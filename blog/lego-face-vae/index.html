<!DOCTYPE html>
<html lang="en">
<head>
  <title>Variational autoencoder for Lego faces - echevarria.io</title>
  <base href="../../"/>
  <link rel="stylesheet" type="text/css" href="css/default.css">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
  <div class="wrapper" id="main-wrapper">
    <div class="nav">
      <a class="home" href="index.html">Ivan Echevarria</a>
      <div class="spacer"></div>
      <a href="photos/index.html">photos</a>
      <a href="code/index.html">projects</a>
      <a href="blog/index.html">writing</a>
      <a href="about.html">about</a>
    </div>
    <div class="content">
      <div class="blog-post">
        <h1>Variational autoencoder for Lego faces</h1>
        <p><img class="img-no-pad" src="blog/lego-face-vae/face-morph-1.png" alt="Lego face morph visualization"/>
        <img class="img-no-pad" src="blog/lego-face-vae/face-morph-2.png" alt="Lego face morph visualization"/>
        <img class="img-no-pad" src="blog/lego-face-vae/face-morph-3.png" alt="Lego face morph visualization"/>
        <img class="img-no-pad" src="blog/lego-face-vae/face-morph-4.png" alt="Lego face morph visualization"/>
        <img src="blog/lego-face-vae/face-morph-5.png" alt="Lego face morph visualization"/>
        <p>
          I spent some of my time off this winter reading 
          <a href="https://github.com/davidADSP">David Foster</a>'s excellent book
          <a href="https://www.oreilly.com/library/view/generative-deep-learning/9781492041931/"><em>Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play</em></a>.
          The book is about generative models: models that learn to describe how
          data sets are structured and that can be used to create
          new data. Some of the specific models it covers include autoencoders,
          generative adversarial networks (GANs), and recurrent neural networks
          (RNNs).
        </p>
        <p>
          I decided to try write a model described in the book on my own, and I
          went with a variational autoencoder (VAE) because they train
          relatively quickly. Because of this, I wouldn't have to wait long to
          figure out if I had made the model right or mangled it.
          I also wanted to do something kind of novel with the model, so I
          compiled a new data set for it to train on: 3800 images of Lego
          minifig faces. The end product can be used for interesting
          applications including generating new Lego faces and morphing smoothly
          between Lego faces.
        </p>
        <p>
          This post will cover the following topics:
          <ul>
            <li>
              <a href="blog/lego-face-vae/index.html#vae-explained">Background information about VAEs</a>
            </li>
            <li>
              <a href="blog/lego-face-vae/index.html#implementation">Implementation details</a>
              (<a href="blog/lego-face-vae/index.html#training-set">training set</a>,
                <a href="blog/lego-face-vae/index.html#model-details">model details</a>)
            </li>
            <li>
              <a href="blog/lego-face-vae/index.html#model-evaluation">Model evaulation</a>
              (<a href="blog/lego-face-vae/index.html#image-reconstruction">image reconstruction</a>,
                <a href="blog/lego-face-vae/index.html#visualizing-latent-space">visualizing the latent space</a>)
            </li>
            <li>
              <b><a href="blog/lego-face-vae/index.html#model-applications">The fun stuff</a></b>
              (<a href="blog/lego-face-vae/index.html#generating-new-images">generating new images</a>,
                <a href="blog/lego-face-vae/index.html#image-morphing">image morphing</a>)
            </li>
          </ul>
        </p>
        <p>
          All of the data and code I used to make the model, plots, and animations
          in this post are publicly available in the project
          <a href="https://github.com/iechevarria/lego-face-VAE">repo</a>.
        </p></p>
        <h2 id="vae-explained">Background: the high-level, handy-wavy explanation of how a variational autoencoder works</h2>

        <p>
          This section covers what a VAE is and what it does at a high level.
          If you already know how a VAE works, you can safely skip this section.
        </p>

        <p>
          An autoencoder (AE) has two main parts: an encoder and a decoder. The
          encoder takes high-dimensional input, an image in this case, and maps it
          to a point in a low-dimensional space. The point is called a latent
          vector, and the low-dimensional space is called the latent space.
          Ideally, each dimension in the latent space contains some kind of
          information. You could imagine that for the case of images of Lego
          faces, one dimension might indicate face color while another might
          indicate eye size, and so on. The following
          shows an example of how an AE's encoder might map images to a latent space:
        </p>

        <div style="text-align: center;">
          <img style="width: 25em;" src="blog/lego-face-vae/tsne-example.jpg" alt="t-SNE plot"/>
        </div>

        <div class="img-caption">
          <li>
            <b>Above</b>: in this simplified example, each image has been assigned a
            position (latent vector) on the graph (latent space) by the encoder
          </li>
        </div>

        <p>
          The decoder, on the other hand, takes a latent vector from the latent
          space as input and expands it back to the form of the original input
          (in this case, an image).
        </p>

        <p>
          During training, the encoder and decoder are connected, and their
          weights are optimized to minimize the difference between each training
          image that enters the encoder and each reconstructed
          image that exits the decoder. The following diagram shows the process
          of an image being compressed into a latent vector by the encoder and
          then being reconstructed by the decoder:
        </p>

        <p><img src="blog/lego-face-vae/vae.svg" alt="Diagram of an autoencoder"/>
        <p>
          A vanilla AE's encoder is connected directly and deterministically to
          the latent vector. Variational autencoders (VAEs) change things up by
          having their encoders define a distribution from which the latent
          vector will be randomly drawn. When a VAE trains, it not only
          minimizes the difference between input images and reconstructions, but
          it also minimizes the divergence between the encoder's distribution
          and the normal distribution.
        </p>
        <p>
          The upshot of this added randomness is that when two nearby vectors
          are put into a VAE's decoder, the reconstructed images are also
          similar. No such assurance exists for a vanilla AE. This property
          gives you the ability to do some fun stuff with a VAE that I'll get
          into later.
        </p>
        <p>
          One last important note before you move on is that the meaning of the latent
          space's dimensions varies from
          model to model. There is no canonical set of image features that's encoded
          into the latent space (for example, the first dimension doesn't always control
          color). Instead, the model is free to choose which features to encode and
          how to encode them so long as it improves its performance. By optimizing
          in this way, the model gains a deep understanding of the underlying
          structure of the data. With this understanding, the model can take on
          complex tasks, including generating novel images it hasn't encountered before.
        </p>
        <p>
          If you want a bit more detail, check out 
          <a href="http://kvfrans.com/">Kevin Frans</a>'
          <a href="http://kvfrans.com/variational-autoencoders-explained/">writing post</a>.
          If you're looking for a more in-depth discussion of the theory and
          math behind VAEs, 
          <em><a href="https://arxiv.org/abs/1606.05908">Tutorial on Variational Autoencoders</a></em>
          by <a href="http://www.carldoersch.com/">Carl Doersch</a> is quite
          thorough.
        </p></p>
        <h2 id="implementation">Implementation details</h2>

        <p>
          This section covers the specifics of the trained VAE model I made for images of Lego faces.
          It includes a description of how I obtained and curated the training set.
          It also includes some discussion of the model's structure and how I trained it.
        </p>

        <h3 id="training-set">The training set</h3>

        <p>
          I got images of Lego faces from two sources:
        </p>

        <ul>
          <li>
            Bricklink, a marketplace for Lego parts and sets. I made a simple
            scraper to get the URL of the largest image for each Lego head on
            the site and to save that image locally.
          </li>
          <li>
            <a href="http://www.bartneck.de/">Christoph Bartneck, Ph.D</a>, a
            researcher and Lego fan. He's taken thousands of photos of Lego
            minifig faces, and they're all excellent. If you're
            interested in Lego minifigures, you should check out his 
            <a href="http://www.minifigure.org/">books</a>. I learned of his
            work by way of an
            <a href="https://qz.com/1405657/how-are-lego-emotions-changing-help-us-find-out/">article</a>
            by <a href="http://www.sadbumblebee.com/">Daniel Wolfe</a>
            about the emotions of Lego faces.
          </li>
        </ul>

        <p>
          Once I had all the images in one place, I wrote some quick scripts to
          make them uniform. For example, several hundred of the Bricklink
          images contained two or three photos side-by-side, so I made a script
          to identify these and split them into individual photos. The scripts I used
          to pull and process the images are in the 
          <a href="https://github.com/iechevarria/lego-face-VAE/tree/master/dataset_scripts">dataset_scripts directory</a>
          in my project repo.
        </p>

        <p>
          I eventually reached the end of what I could reasonably automate and
          had to manually pick through the images. I removed several hundred
          images for quality issues including bad lighting and strange camera angles.
          Several hundred others did not contain faces. In the end, I had
          a 3800 image training set, and I was a bit concerned that its small
          size would preclude me from doing anything interesting with it.
        </p>

        <h3 id="model-details">Model details</h3>

        <p>
          I trained the model on images from the training set downsized from
          128x128 to 64x64 (only 4 MB of training data total). The VAE's encoder
          has 5 convolutional layers and its decoder
          has 5 convolutional transpose layers. I used 200 dimensions for the
          latent vector. The overall model has
          2.7 million parameters and weighs in at 10 MB when trained. I trained
          it for two hours on a Google Colab GPU instance. 
        </p>

        <p>
          I made no special effort to improve the VAE's performance or to choose
          an optimal architecture. I recognize that the number of layers and the
          size of the latent vector is probably overkill for this task. My sole
          aim was to get a usable model to do the fun stuff below.
        </p>

        <p>
          My VAE implementation and some associated utilities are in the
          <a href="https://github.com/iechevarria/lego-face-VAE/tree/master/ml">ml</a>
          directory of my project repo. The code is largely based on snippets
          from the book <em>Generative Deep Learning</em> by David Foster.
          I also put a pretrained model in the
          <a href="https://github.com/iechevarria/lego-face-VAE/tree/master/trained_model">trained_model</a>
          directory of my project repo.
        </p>

        <h2 id="model-evaluation">Model evaluation</h2>

        <p>
          This section covers two ways to informally evaluate the
          model's performance and its understanding of the underlying
          structure of the training set.
        </p>

        <h3 id="image-reconstruction">Image reconstruction</h3>

        <p>
          A quick way to assess the quality of a trained VAE model is to compare
          raw images with their reconstructed counterparts. The following shows
          images of Lego faces and their reconstructions:
        </p>

        <p><img class="img-no-pad" src="blog/lego-face-vae/reconstruction.png" alt="Lego face reconstructions"/>
        <div class="img-caption">
          <ul>
            <li>
              <b>Top</b>: raw input images
            </li>
            <li>
              <b>Bottom</b>: reconstructions created by converting the raw
              input image into a latent vector with the VAE encoder and then
              converting this latent vector into an image with the VAE decoder 
            </li>
          </ul>
        </div>
        <p>
          The results are okay! At a high level, it looks like the model gets
          that a minifig's face should have eyes, eyebrows, and a mouth. It
          also renders the shape, color, and lighting of the heads with fair
          accuracy.
        </p>
        <p>
          More interesting than what the model gets right is what it gets wrong.
          Across the board, it has trouble with expressions. Take the first
          raw image: the face is an angry, mustached, and goateed. Meanwhile,
          its reconstruction lacks any kind of facial hair, and it has a an
          expression that's closer to
          <a href="https://tvtropes.org/pmwiki/pmwiki.php/Main/DreamWorksFace">DreamWorks Face</a>
          than anger.
        </p>
        <p>
          The last raw image and reconstruction go in the opposite direction:
          from happy to angry, or at least displeased. The raw image is of a simple
          smiley face with eyebrows. You'd think it'd be hard to mess up, but
          its reconstruction is much sterner, with a frown that approaches a
          sneer.
        </p>
        <p>
          If your initial reaction to all this is "okay, so you managed to take
          4 MB of images and store it in 10 MB of model and still end up with
          terrible reproductions" - you're right! Luckily, VAEs can do
          more than encode and decode single images, and I'll get to that in the
          <a href="blog/lego-face-vae/index.html#model-applications">model applications</a>
          section.
        </p></p>
        <h3 id="visualizing-latent-space">Visualizing the latent space</h3>

        <p>
          Another way to build some understanding of a trained VAE model is to see where
          in the latent space it thinks each of the images belongs. If we can
          plot each image's location in the latent space, we can begin to
          understand which images the model thinks are similar.
        <p>
          You can start by putting all the images into the VAE's encoder and getting
          latent vectors out (which each represent an image's location in the latent space).
          However, each of the latent vectors have 200 dimensions. It's
          inadvisable to plot data in three dimensions, let alone 200.
        </p>
        <p>
          Fortunately, there exist many methods for dimensionality reduction.
          One such method is <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-SNE</a>
          (t-distributed stochastic neighbor embedding). I'm not going to get
          into the details of how it works because that's well out of scope for
          this post, but the upshot is this: you can use t-SNE to take
          200-dimensional vectors and to turn them into 2-dimensional vectors
          while minimizing the amount of information you lose.
        </p>
        <p>
          I put all the images in the training set through the VAE encoder to
          get their 200 dimension latent vectors. I then used t-SNE to turn
          these 200 dimension vectors into 2-dimensional vectors. Finally, I
          plotted the input images at these coordinates:
        </p>
        <a href="blog/lego-face-vae/tsne-all-lg.jpg">
          <img class="img-really-no-pad" src="blog/lego-face-vae/tsne-all-sm.jpg" alt="t-SNE plot of all the images"/>
        </a>
        <div class="img-note">click the image above to see a full-size version</div>

        <p>
          The following are some crops from the full plot that demonstrate how
          faces with similar features cluster together:
        </p>
        <div style="text-align: center;">
          <img class="img-horiz" style="width: 11em; border: 1px solid black;" src="blog/lego-face-vae/tsne-crop-1.jpg" alt="crop from t-SNE plot"/>
          <img class="img-horiz" style="width: 11em; border: 1px solid black;" src="blog/lego-face-vae/tsne-crop-2.jpg" alt="crop from t-SNE plot"/>
          <img class="img-horiz" style="width: 11em; border: 1px solid black;" src="blog/lego-face-vae/tsne-crop-3.jpg" alt="crop from t-SNE plot"/>
        </div>
        <p>
          In the first image, you can see that faces with black hair and
          mustaches are in a group. In the second image, four heads
          with dark balaclavas and white headbands sit close to one another. In
          the last image, heads with orange visors (and a jack-o-lantern?) are
          clustered together. The
          <a href="blog/lego-face-vae/tsne-all-lg.jpg">full-size image</a>
          has other little clusters like these.
        </p>
        <p>
          A word of warning: it's important that you don't to assign too much
          meaning to this visualization or really any made using t-SNE. Because the
          latent vectors have been compressed so severely, you are unable to see
          many of the distinctions that the model actually makes. Still, you can
          begin to build some intuition about what the model understands about
          faces. And I think it looks neat, too!
        </p>

        <h2 id="model-applications">Model applications (the fun stuff)</h2>

        <p>
          This section covers two of neat things that you can do with the trained
          VAE: generating new images and morphing between existing images.
        </p>

        <h3 id="generating-new-images">Generating new images</h3>

        <p>
          A VAE is called a generative model because you can use it to create new
          images it's never seen before. So let's make some! The mechanics are
          pretty simple. First, you choose a random vector from the latent space.
          You then pass this vector into the VAE's decoder and get a brand new 
          image out that most likely does not exist in the training set. The
          following image depicts this process:
        </p>

        <div style="text-align: center;">
          <img class="img-horiz img-really-no-pad" style="width: 18em;" src="blog/lego-face-vae/generative-example-1.jpg" alt="how new images are generated"/>
          <img class="img-horiz img-really-no-pad" style="width: 18em;" src="blog/lego-face-vae/generative-example-2.jpg" alt="how new images are generated"/>
        </div>

        <div class="img-caption">
          <ul>
            <li>
              <b>Left</b> (top on mobile): we choose random vectors in
              the latent space (each represented by <b style="color: blue;">o</b>)
            <li>
              <b>Right</b> (bottom on mobile): the VAE's decoder turns the
              chosen vectors into images
            </li>
          </ul>
        </div>

        <p>
          So how well does this work in practice? The results range from
          nightmarish to somewhat convincing. The following are faces generated
          randomly by the process described above:
        </p>

        <p><img class="img-really-no-pad" src="blog/lego-face-vae/random-1.png" alt="Randomly generated Lego faces"/>
        <img class="img-really-no-pad" src="blog/lego-face-vae/random-3.png" alt="Randomly generated Lego faces"/>
        <img class="img-really-no-pad" src="blog/lego-face-vae/random-4.png" alt="Randomly generated Lego faces"/>
        <img class="img-really-no-pad" src="blog/lego-face-vae/random-5.png" alt="Randomly generated Lego faces"/>
        <img class="img-really-no-pad" src="blog/lego-face-vae/random-6.png" alt="Randomly generated Lego faces"/>
        <p>
          Nearly all the generated images have the standard features of a Lego
          face: eyes, eyebrows, and a mouth. They almost all get the shape of a
          Lego head right, too. But geez, when the VAE misses, it really misses.
          Some of the generated images put the facial features in the wrong place
          entirely while others are essentially just noise.
        </p></p>
        <h3 id="image-morphing">Image morphing</h3>

        <p>
          Look at the five following images. Hopefully you noticed that they're
          actually animated and morphing between two different
          faces.
        </p>

        <div style="text-align: center;">
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="blog/lego-face-vae/face-morph-1.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="blog/lego-face-vae/face-morph-2.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="blog/lego-face-vae/face-morph-3.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="blog/lego-face-vae/face-morph-4.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="blog/lego-face-vae/face-morph-5.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>  
        </div>

        <div class="img-note" style="margin-top: 0.5em;">
          watch closely: these faces are animated!
        </div>

        <p>
          The process of morphing between images is a little different from
          generating new images. Instead
          of jumping straight into the decoder, you actually need to use the
          encoder, too. There are four main steps:
          <ol>
            <li>
              Choose two images that you want to morph between
            </li>
            <li>
              Put both images into the VAE's encoder and get a latent vector out for each
            </li>
            <li>
              Choose several intermediate vectors between the two latent vectors
            </li>
            <li>
              Take the intermediate vectors and pass them into the VAE's decoder to generate images
            </li>
          </ol>
        </p>

        <p>
          The following image depicts this process:
        </p>

        <div style="text-align: center;">
          <img class="img-horiz img-really-no-pad" style="width: 18em;" src="blog/lego-face-vae/morph-example-1.jpg" alt="how a face morph works"/>
          <img class="img-horiz img-really-no-pad" style="width: 18em;" src="blog/lego-face-vae/morph-example-2.jpg" alt="how a face morph works"/>
        </div>

        <div class="img-caption">
          <ul>
            <li>
              <b>Left</b> (top on mobile): two initial images are assigned
              latent vectors by the VAE's encoder. You then choose several 
              intermediate vectors (each represented by
              <b style="color: blue;">o</b>) between these two images'
              latent vectors
            </li>
            <li>
              <b>Right</b> (bottom on mobile): the VAE's decoder turns the
              chosen vectors into images
            </li>
          </ul>
        </div>

        <p>
          Note that none of the faces in the transition between the two input
          faces exist in the training set. These are brand new Lego faces that
          nobody has ever seen before!
        </p>

        <p>
          Let's take a closer look at two of these morphs:
        </p>

        <p><img class="img-no-pad" src="blog/lego-face-vae/face-morph-3.png" alt="Lego face morph visualization"/>
        <p>
          Notice how every single intermediate face is completely plausible on
          its own. I don't think I'd be able to guess which of these is real
          and which of these is fake if I didn't already know. Going from left
          to right, you can see the mouth naturally close and the eyebrows
          widen.
        </p>
        <img class="img-no-pad" src="blog/lego-face-vae/face-morph-4.png" alt="Lego face morph visualization"/>
        <p>
          I think this morph is particularly neat because you can see the first
          face's eyebrows move up and spread out to become hair. Additionally,
          you can see the mustache subtly grow and the smile slowly contract at
          each step.
        </p>
        <p>
          Be aware that not all transitions are as clean or as neat as the ones
          shown above - I specifically chose these because they look good. Still, if you
          run through some combinations on your own, it's likely that you can find morphs
          that look even better than these.
        </p></p>
        <h2>Closing thoughts</h2>

        <p>
          I hope this was an interesting look at my variational autoencoder
          trained on Lego faces! Again,
          all of the data and code I used to make the model, plots, and animations
          in this post are publicly available in the project
          <a href="https://github.com/iechevarria/lego-face-VAE">repo</a>.
        </p>

        <p>
          The project repo also includes a quickstart guide to running the code
          on <a href="https://colab.research.google.com/">Google Colab</a>, a
          free environment to run Jupyter Notebooks with free optional GPU/TPU
          instances. It takes less than a minute to set up, and once it's ready,
          you can generate your own Lego faces and make your own face morph
          animations.
        </p>

        <p>
          Finally, if you find any errors in this post
          or if you just want to chat, feel free to reach out via email at
          <a href="mailto:ivan@echevarria.io">ivan@echevarria.io</a>. Thanks for
          reading!
        </p>

        <p class="note">
          If you want to buy the book <em>Generative Deep Learning</em> from 
          Amazon and you want to throw a buck my way, here's an 
          <a href="https://amzn.to/2RBqkBl">affiliate link</a>. I didn't write
          this post to sell you anything, I just liked the book. None of the
          other links in this post are affiliate links.
        </p>

        <p><em>
          Thanks to <a href="http://devinlogan.org/">Devin Logan</a> for her
          incredibly helpful input on earlier drafts of this post
        </em></p>
        <p class="footer">Ivan Echevarria - February 3, 2020</p>
      </div>
    </div>
  </div>
  <script data-goatcounter="https://ivan.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
</body>
</html>