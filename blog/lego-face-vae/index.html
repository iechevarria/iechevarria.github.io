<head>
  <title>Variational autoencoder trained on Lego faces - echevarria.io</title>
  <base href="../../"/>
  <link rel="stylesheet" type="text/css" href="css/default.css">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>

<body>
  <div class="wrapper" id="main-wrapper">
    <div class="nav">
      <a class="home" href="index.html">Ivan Echevarria</a>
      <div class="spacer"></div>
      <a href="photos/index.html">photos</a>
      <a href="code/index.html">code</a>
      <a href="blog/index.html">blog</a>
      <a href="about.html">about</a>
    </div>
    <div class="content">

      <div class="blog-post">
        <h1>Variational autoencoder for Lego faces</h1>
        <img class="img-no-pad" src="./blog/lego-face-vae/face-morph-1.png" alt="Lego face morph visualization"/>
        <img class="img-no-pad" src="./blog/lego-face-vae/face-morph-2.png" alt="Lego face morph visualization"/>
        <img class="img-no-pad" src="./blog/lego-face-vae/face-morph-3.png" alt="Lego face morph visualization"/>
        <img class="img-no-pad" src="./blog/lego-face-vae/face-morph-4.png" alt="Lego face morph visualization"/>
        <img class="img-no-pad" src="./blog/lego-face-vae/face-morph-5.png" alt="Lego face morph visualization"/>
        <p class="note">
          Note: this post contains affiliate links. All affiliate links are
          labeled "affiliate link". A non-affiliate link labeled
          "non-affiliate link" immediately follows each affiliate link.
        </p>
        <p>
          I spent some of my time off this winter reading 
          <a href="https://github.com/davidADSP">David Foster</a>'s excellent book
          <em>Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play</em>
          (<a href="https://amzn.to/2RBqkBl">affiliate link</a>,
          <a href="https://www.amazon.com/Generative-Deep-Learning-Teaching-Machines/dp/1492041947">non-affiliate link</a>)
          and working through some of the exercises in it.
        </p>
    
        <h3>The high-level, handy-wavy explanation of how a VAE works</h3>
        <p>
          An autoencoder (AE) has two main parts: an encoder and a decoder. The
          encoder takes high-dimensional input (e.g., an image) and compresses
          it into a low-dimenstional vector representation which I'll refer to
          as a latent vector. Meanwhile, the decoder takes a latent vector as
          input and expands it back to form of the original input.
        </p>
        <p>
          For training, the encoder and decoder are connected, and their weights
          are optimized to minimize the difference between
          training image that enters the encoder and each reconstructed
          image that exits the decoder. The following diagram shows the process
          of an image being compressed into a latent vector by the encoder and
          then being reconstructed by the decoder:
        </p>
        <img src="./blog/lego-face-vae/vae.svg" alt="Diagram of an autoencoder"/>
        <p>
          A variational autencoder (VAE) changes things up by having the encoder
          define a distribution from which the latent vector will be randomly
          drawn. When a VAE trains, it not only minimizes the difference between
          input images and reconstructions, but it also minimizes the divergence
          between the encoder's distribution and the normal distribution.
        </p>
        <p>
          The upshot of all this is that when two nearby vectors are put into a
          VAE's decoder, the reconstructed images are also similar. No such
          assurance exists for a vanilla AE. This property gives us the ability
          to do some fun stuff with a VAE, including morphing between images and
          generating plausible new images that the VAE hasn't seen before.
        </p>
        <p>
          If you're looking for a bit more detail, check out 
          <a href="http://kvfrans.com/">Kevin Frans</a>'
          <a href="http://kvfrans.com/variational-autoencoders-explained/">blog post</a>.
          If you want a more in-depth discussion of the theory and math behind
          VAEs, 
          <em><a href="https://arxiv.org/abs/1606.05908">Tutorial on Variational Autoencoders</a></em>
          by <a href="http://www.carldoersch.com/">Carl Doersch</a> is quite
          thorough.
        </p>


        <h3>The training set</h3>
        <p>
          I threw together some quick scripts to pull images of Lego faces from
          two sources: Bricklink and Quartz.
        </p>

        <h3>Results</h3>
        <p>
          The following are random images from the training set 
        </p>
  
        <h3>Results</h3>
        <p>
          I trained a VAE for an hour or so on Google Colab.
        </p>
        <p>
          The following are random images from the training set 
        </p>
  
        <div style="text-align: center;">
          <video width="120" height="120" autoplay loop>
            <source src="./blog/lego-face-vae/face-morph-1.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop>
            <source src="./blog/lego-face-vae/face-morph-2.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop>
            <source src="./blog/lego-face-vae/face-morph-3.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop>
            <source src="./blog/lego-face-vae/face-morph-4.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop>
            <source src="./blog/lego-face-vae/face-morph-5.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>   
        </div>

        <h3>Takeaways</h3>
        <p>

        </p>

        <p class="footer">Ivan Echevarria - January 20, 2020 </p>
      </div>
    </div>
  </div>

  <script data-goatcounter="https://ivan.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
</body>
	  