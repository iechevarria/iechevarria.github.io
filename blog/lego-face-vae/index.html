<head>
  <title>Variational autoencoder for Lego faces - echevarria.io</title>
  <base href="../../"/>
  <link rel="stylesheet" type="text/css" href="css/default.css">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>

<body>
  <div class="wrapper" id="main-wrapper">
    <div class="nav">
      <a class="home" href="index.html">Ivan Echevarria</a>
      <div class="spacer"></div>
      <a href="photos/index.html">photos</a>
      <a href="code/index.html">code</a>
      <a href="blog/index.html">blog</a>
      <a href="about.html">about</a>
    </div>
    <div class="content">

      <div class="blog-post">
        <h1>Variational autoencoder for Lego faces</h1>
        <img class="img-no-pad" src="./blog/lego-face-vae/face-morph-1.png" alt="Lego face morph visualization"/>
        <img class="img-no-pad" src="./blog/lego-face-vae/face-morph-2.png" alt="Lego face morph visualization"/>
        <img class="img-no-pad" src="./blog/lego-face-vae/face-morph-3.png" alt="Lego face morph visualization"/>
        <img class="img-no-pad" src="./blog/lego-face-vae/face-morph-4.png" alt="Lego face morph visualization"/>
        <img src="./blog/lego-face-vae/face-morph-5.png" alt="Lego face morph visualization"/>
        <p>
          I spent some of my time off this winter reading 
          <a href="https://github.com/davidADSP">David Foster</a>'s excellent book
          <a href="https://www.oreilly.com/library/view/generative-deep-learning/9781492041931/"><em>Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play</em></a>
          and working through some of the exercises in it.
        </p>
        <p>
          I decided to try write a model from the book from scratch, and I went with
          a variational autoencoder (VAE) because they train fairly quickly so I
          could figure out whether I had done it right or mangled it without
          waiting too long.
        </p>
        <p>
          I also wanted to do something kind of novel with the model, so I
          compiled a data set: 3800 images of Lego minifig faces.
          At a certain point, working with <a href="">MNIST</a>
          or any other pre-made training set isn't fun.
          The result is assured from the start, so there's little to no
          adventure.
        </p>

        <!-- <p>Table of contents:</p>
        <ul>
          <li><a href="./blog/lego-face-vae/index.html#vae-explained">VAEs explained</a></li>
          <li><a href="./blog/lego-face-vae/index.html#training-set">Training set</a></li>
          <li><a href="./blog/lego-face-vae/index.html#results">Results</a></li>
          <li><a href="./blog/lego-face-vae/index.html#image-reconstructions">Image reconstructions</a></li>
          <li><a href="./blog/lego-face-vae/index.html#face-morphing">Face morphing</a></li>
          <li><a href="./blog/lego-face-vae/index.html#generating-new-faces">Generating new faces</a></li>
        </ul> -->
  
        <h2 id="vae-explained">The high-level, handy-wavy explanation of how a variational autoencoder works</h2>
        <p>
          An autoencoder (AE) has two main parts: an encoder and a decoder. The
          encoder takes high-dimensional input (e.g., an image) and compresses
          it into a low-dimenstional vector representation called a latent or
          representation vector. You can think of a latent vector as a point
          in a space. I'll refer to this space as the latent space. The
          following image depicts how an encoder might map images in latent
          space:
        </p>
        <div style="text-align: center;">
          <img style="width: 25em;" src="./blog/lego-face-vae/tsne-example.jpg" alt="t-SNE plot"/>
        </div>
        <div class="img-caption">
          <b>Above</b>: this shows 
        </div>
        <p>
          Meanwhile, the decoder takes a latent vector as input and expands it
          back to form of the original input.  
        </p>
        <p>
          During training, the encoder and decoder are connected, and their
          weights are optimized to minimize the difference between training
          image that enters the encoder and each reconstructed
          image that exits the decoder. The following diagram shows the process
          of an image being compressed into a latent vector by the encoder and
          then being reconstructed by the decoder:
        </p>
        <img src="./blog/lego-face-vae/vae.svg" alt="Diagram of an autoencoder"/>
        <p>
          A vanilla AE's encoder is connected directly and deterministically to
          the latent vector. Variational autencoders (VAEs) change things up by
          having their encoders define a distribution from which the latent
          vector will be randomly drawn. When a VAE trains, it not only
          minimizes the difference between input images and reconstructions, but
          it also minimizes the divergence between the encoder's distribution
          and the normal distribution.
        </p>
        <p>
          The upshot of this added randomness is that when two nearby vectors
          are put into a VAE's decoder, the reconstructed images are also
          similar. No such assurance exists for a vanilla AE. This property
          gives us the ability to do some fun stuff with a VAE that we'll get
          into later.
        </p>
        <p>
          If you want a bit more detail, check out 
          <a href="http://kvfrans.com/">Kevin Frans</a>'
          <a href="http://kvfrans.com/variational-autoencoders-explained/">blog post</a>.
          If you're looking for a more in-depth discussion of the theory and
          math behind VAEs, 
          <em><a href="https://arxiv.org/abs/1606.05908">Tutorial on Variational Autoencoders</a></em>
          by <a href="http://www.carldoersch.com/">Carl Doersch</a> is quite
          thorough.
        </p>

        <h2 id="training-set">The training set</h2>
        <p>
          I threw together some quick scripts to pull images of Lego faces from
          two sources: Bricklink and Quartz. For the uninitiated, Bricklink is
          a marketplace for Lego parts and sets. I made a simple scraper to get
          the URL of the largest image for each Lego head on the site and to
          save that image locally.
        <p>
          Quartz, meanwhile, published an
          <a href="https://qz.com/1405657/how-are-lego-emotions-changing-help-us-find-out/">article/piece of interactive software</a>
          by <a href="https://twitter.com/sadbumblebee">Daniel Wolfe</a>
          about the changing emotions of Lego faces. It asked users to label the
          emotions in around 1600 Lego faces to see how they've changed since 
          1975. The photos were taken by
          <a href="http://www.bartneck.de/">Christoph Bartneck, Ph.D</a>, a
          researcher and Lego fan, and they're all excellent. Fortunately, the
          Quartz application had a very simple-to-use API that allowed me to
          pull the images without resorting to full-blown scraping. (Note:
          scraping HTML directly should be your last resort - be a good internet
          citizen and use APIs when they exist.)
        </p>
        <p>
          Once I had all the images in one place, I made some quick scripts to
          make them uniform. For example, several hundred of the Bricklink
          images contained two or three photos side-by-side, so I made a script
          to identify and split them into individual photos. The scripts I used
          to pull and process the images are in the 
          <a href="https://github.com/iechevarria/lego-face-VAE/tree/master/dataset_scripts">dataset_scripts directory</a>
          in my project repo.
        </p>
        <p>
          I eventually reached the end of what I could reasonably automate and
          had to manually pick through the images. I removed several hundred
          images for quality issues including bad lighting and strange angles.
          Several hundred others did not contain faces. In the end, I came out
          with a 3800 image training set, and I had some concern that its small
          size would preclude me from doing anything interesting with it.
        </p>
        
        <h2 id="results">Results</h2>
        <p>
          I trained the model on images from the training set downsized from
          128x128 to 64x64 (only 4 MB of training data total). The VAE's encoder
          has 5 convolutional layers and its decoder
          has 5 convolutional transpose layers. I used 200 dimensions for the
          latent vector. The overall model has
          2.7 million parameters and weighs in at 10 MB when trained. I trained
          it for two hours on a Google Colab GPU instance. 
        </p>
        <p>
          I made no special effort to improve the VAE's performance or to choose
          an optimal architecture. I recognize the the number of layers and the
          size of the latent vector is probably overkill for this task. My sole
          aim was to get a usable model to do the fun stuff below.
        </p>

        <h3 id="image-reconstructions">Image reconstructions</h3>
        <p>
          The following image shows images and their reconstructions:
        </p>
        <img class="img-no-pad" src="./blog/lego-face-vae/reconstruction.png" alt="Lego face reconstructions"/>
        <div class="img-caption">
          <ul>
            <li>
              <b>Top</b>: raw input images
            </li>
            <li>
              <b>Bottom</b>: reconstructions created by converting the raw
              input image into a latent vector with the VAE encoder and then
              converting this latent vector into an image with the VAE decoder 
            </li>
          </ul>
        </div>
        <p>
          The results are okay! At a high level, it looks like the model gets
          that a minifig's face should have eyes, eyebrows, and a mouth. It
          also renders the shape, color, and lighting of the heads with fair
          accuracy.
        </p>
        <p>
          More interesting than what the model gets right is what it gets wrong.
          Across the board, it has trouble with expressions. Take the first
          raw image - the face is an angry, mustached, and goateed. Meanwhile,
          its reconstruction lacks any kind of facial hair, and it has a an
          expression that's closer to
          <a href="https://tvtropes.org/pmwiki/pmwiki.php/Main/DreamWorksFace">DreamWorks Face</a>
          than anger.
        </p>
        <p>
          The last raw image and reconstruction go in the opposite direction:
          from happy to angry, or at least displeased. The raw image is a simple
          smiley face with eyebrows. You'd think it'd be hard to mess up, but
          its reconstruction is much sterner, with a frown that approaches a
          sneer.
        </p>
        <p>
          If your initial reaction to all this is "okay, so you managed to take
          4 MB of images and store it in 10 MB of model and still end up with
          terrible reproductions" - you're right! Luckily for us, VAEs can do
          more than encode and decode single images.
        </p>

        <h3 id="face-morphing">Face morphing</h3>
        <p>
          Look at the five following images. Seriously, look at them for a
          bit. Hopefully you noticed that they're morphing between two different
          faces. Neat, right?
        </p>
        <div style="text-align: center;">
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="./blog/lego-face-vae/face-morph-1.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="./blog/lego-face-vae/face-morph-2.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="./blog/lego-face-vae/face-morph-3.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="./blog/lego-face-vae/face-morph-4.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="./blog/lego-face-vae/face-morph-5.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>  
        </div>
        <br>
        <p>
          The mechanics are pretty simple. First, we take two images that we
          want to morph between. We put them into the VAE's encoder and get
          a latent vector out for each. Recall from earlier that the latent
          vector is essentially a point in space. Next, we take the two vectors
          and get several intermediate vectors on the line between them.
          Finally, we take all the vectors and pass them into the VAE's decoder
          to generate images. The following image depicts this process:
        </p>
        <div style="text-align: center;">
          <img class="img-horiz img-really-no-pad" style="width: 18em;" src="./blog/lego-face-vae/morph-example-1.jpg" alt="t-SNE plot"/>
          <img class="img-horiz img-really-no-pad" style="width: 18em;" src="./blog/lego-face-vae/morph-example-2.jpg" alt="t-SNE plot"/>
        </div>
        <div class="img-caption">
          <ul>
            <li>
              <b>Left</b> (top on mobile): two initial images are assigned
              latent vectors by the VAE's encoder. We then choose several 
              intermediate vectors (each represented by
              <b style="color: blue;">o</b>) between these two images'
              latent vectors
            </li>
            <li>
              <b>Right</b> (bottom on mobile): the VAE's decoder turns the
              chosen vectors into images
            </li>
          </ul>
        </div>
        <p>
          Note that none of the faces in the transition between the two input
          faces exist in the training set. These are brand new Lego faces that
          nobody has ever seen before!
        </p>
        <p>
          The randomness ensures that it's a smooth transition
        </p>

        <h3 id="generating-new-faces">Generating new faces</h3>
        <p>
          Generating new faces follows a similar process to face morphing.
          Instead of starting out with the encoder, though, we jump right into
          the decoder. First, we get a vector with random values. Then we pass
          the vector into the VAE's decoder and get a brand new image out that
          most likely does not exist in the training set. The following image
          depicts this process:
        </p>
        <div style="text-align: center;">
          <img class="img-horiz img-really-no-pad" style="width: 18em;" src="./blog/lego-face-vae/generative-example-1.jpg" alt="t-SNE plot"/>
          <img class="img-horiz img-really-no-pad" style="width: 18em;" src="./blog/lego-face-vae/generative-example-2.jpg" alt="t-SNE plot"/>
        </div>
        <div class="img-caption">
          <ul>
            <li>
              <b>Left</b> (top on mobile): we choose random vectors in
              the latent space (each represented by <b style="color: blue;">o</b>)
            <li>
              <b>Right</b> (bottom on mobile): the VAE's decoder turns the
              chosen vectors into images
            </li>
          </ul>
        </div>
        <p>
          So how well does this work in practice? The results range from
          nightmarish to somewhat convincing. The following are faces generated
          randomly by the process described above:
        </p>
        <img class="img-really-no-pad" src="./blog/lego-face-vae/random-1.png" alt="Randomly generated Lego faces"/>
        <img class="img-really-no-pad" src="./blog/lego-face-vae/random-3.png" alt="Randomly generated Lego faces"/>
        <img class="img-really-no-pad" src="./blog/lego-face-vae/random-4.png" alt="Randomly generated Lego faces"/>
        <img class="img-really-no-pad" src="./blog/lego-face-vae/random-5.png" alt="Randomly generated Lego faces"/>
        <img class="img-really-no-pad" src="./blog/lego-face-vae/random-6.png" alt="Randomly generated Lego faces"/>
        <p>
          Nearly all the generated images have the standard features of a Lego
          face: eyes, eyebrows, and a mouth. They the shape of a Lego head
          right, too. 
        </p>
  
        <h3 id="generating-new-faces">Visualizing the latent space</h3>
        <img class="img-really-no-pad" src="./blog/lego-face-vae/tsne-all-sm.jpg" alt="t-SNE plot of all the images"/>
        <p>
          The following are some crops from the full plot that demnstrate how
          faces with similar features cluster together:
        </p>
        <div style="text-align: center;">
          <img class="img-horiz" style="width: 11em; box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2);" src="./blog/lego-face-vae/tsne-crop-1.jpg" alt="t-SNE plot"/>
          <img class="img-horiz" style="width: 11em; box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2);" src="./blog/lego-face-vae/tsne-crop-2.jpg" alt="t-SNE plot"/>
          <img class="img-horiz" style="width: 11em; box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2);" src="./blog/lego-face-vae/tsne-crop-3.jpg" alt="t-SNE plot"/>
        </div>
        <p>
          The <a href="./blog/lego-face-vae/tsne-all-lg.jpg">full-size image</a>
          has other little clusters like these.
        </p>

        <h2>Parting thoughts</h2>
        <p>
        </p>

        <p class="note">
          If you want to buy the book <em>Generative Deep Learning</em> from 
          Amazon and you want to throw a buck my way, here's an 
          <a href="https://amzn.to/2RBqkBl">affiliate link</a>. I didn't write
          this post to sell you anything, I just liked the book. None of the
          other links in this post are affiliate links.
        </p>

        <p class="footer">Ivan Echevarria - January 29, 2020 </p>
      </div>
    </div>
  </div>

  <script data-goatcounter="https://ivan.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
</body>
	  