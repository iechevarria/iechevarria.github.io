<head>
  <title>Variational autoencoder for Lego faces - echevarria.io</title>
  <base href="../../"/>
  <link rel="stylesheet" type="text/css" href="css/default.css">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>

<body>
  <div class="wrapper" id="main-wrapper">
    <div class="nav">
      <a class="home" href="index.html">Ivan Echevarria</a>
      <div class="spacer"></div>
      <a href="photos/index.html">photos</a>
      <a href="code/index.html">code</a>
      <a href="blog/index.html">blog</a>
      <a href="about.html">about</a>
    </div>
    <div class="content">

      <div class="blog-post">
        <h1>Variational autoencoder for Lego faces</h1>
        <img class="img-no-pad" src="./blog/lego-face-vae/face-morph-1.png" alt="Lego face morph visualization"/>
        <img class="img-no-pad" src="./blog/lego-face-vae/face-morph-2.png" alt="Lego face morph visualization"/>
        <img class="img-no-pad" src="./blog/lego-face-vae/face-morph-3.png" alt="Lego face morph visualization"/>
        <img class="img-no-pad" src="./blog/lego-face-vae/face-morph-4.png" alt="Lego face morph visualization"/>
        <img src="./blog/lego-face-vae/face-morph-5.png" alt="Lego face morph visualization"/>
        <p>
          I spent some of my time off this winter reading 
          <a href="https://github.com/davidADSP">David Foster</a>'s excellent book
          <a href="https://www.oreilly.com/library/view/generative-deep-learning/9781492041931/"><em>Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play</em></a>.
        </p>
        <p>
          I decided to try write a model described in the book on my own, and I
          went with a variational autoencoder (VAE) because they train
          relatively quickly. Because of this, I could figure out whether I had
          made the model right or mangled it without waiting too long.
        </p>
        <p>
          I also wanted to do something kind of novel with the model, so I
          compiled a new data set for the model to train on: 3800 images of Lego
          minifig faces.
        </p>
        <p>
          All of the data and code I used to make the model, plots, and animations
          in this post are publicly available in the project
          <a href="https://github.com/iechevarria/lego-face-VAE">repo</a>.
        </p>


        <!-- <p>Table of contents:</p>
        <ul>
          <li><a href="./blog/lego-face-vae/index.html#vae-explained">VAEs explained</a></li>
          <li><a href="./blog/lego-face-vae/index.html#training-set">Training set</a></li>
          <li><a href="./blog/lego-face-vae/index.html#results">Results</a></li>
          <li><a href="./blog/lego-face-vae/index.html#image-reconstructions">Image reconstructions</a></li>
          <li><a href="./blog/lego-face-vae/index.html#face-morphing">Face morphing</a></li>
          <li><a href="./blog/lego-face-vae/index.html#generating-new-faces">Generating new faces</a></li>
        </ul> -->
  
        <h2 id="vae-explained">The high-level, handy-wavy explanation of how a variational autoencoder works</h2>
        <p>
          An autoencoder (AE) has two main parts: an encoder and a decoder. The
          encoder takes high-dimensional input (in this case an image) maps it
          to a point in a low-dimensional space. The point is called a latent
          vector, and the low-dimensional space is called the latent space.
          Ideally, each coordinate in the latent space encodes some kind of
          information - we could imagine that for the case of Lego faces, one
          coordinate might indicate face color while another might indicate
          whether or not a face has glasses. The following shows an example of
          how an AE's encoder might map faces to a latent space:
        </p>
        <div style="text-align: center;">
          <img style="width: 25em;" src="./blog/lego-face-vae/tsne-example.jpg" alt="t-SNE plot"/>
        </div>
        <div class="img-caption">
          <b>Above</b>: In this example, each face in
          put into the encoder, and the encoder outputs two coordinates. The
          face is then plotted at these assigned coordinates. The
          graph where all the faces are plotted is the latent space.
        </div>
        <p>
          Meanwhile, the decoder takes a latent vector from the latent space as
          input and expands it back to form of the original input.  
        </p>
        <p>
          During training, the encoder and decoder are connected, and their
          weights are optimized to minimize the difference between training
          image that enters the encoder and each reconstructed
          image that exits the decoder. The following diagram shows the process
          of an image being compressed into a latent vector by the encoder and
          then being reconstructed by the decoder:
        </p>
        <img src="./blog/lego-face-vae/vae.svg" alt="Diagram of an autoencoder"/>
        <p>
          A vanilla AE's encoder is connected directly and deterministically to
          the latent vector. Variational autencoders (VAEs) change things up by
          having their encoders define a distribution from which the latent
          vector will be randomly drawn. When a VAE trains, it not only
          minimizes the difference between input images and reconstructions, but
          it also minimizes the divergence between the encoder's distribution
          and the normal distribution.
        </p>
        <p>
          The upshot of this added randomness is that when two nearby vectors
          are put into a VAE's decoder, the reconstructed images are also
          similar. No such assurance exists for a vanilla AE. This property
          gives us the ability to do some fun stuff with a VAE that we'll get
          into later.
        </p>
        <p>
          If you want a bit more detail, check out 
          <a href="http://kvfrans.com/">Kevin Frans</a>'
          <a href="http://kvfrans.com/variational-autoencoders-explained/">blog post</a>.
          If you're looking for a more in-depth discussion of the theory and
          math behind VAEs, 
          <em><a href="https://arxiv.org/abs/1606.05908">Tutorial on Variational Autoencoders</a></em>
          by <a href="http://www.carldoersch.com/">Carl Doersch</a> is quite
          thorough.
        </p>
        <p>
          My VAE implementation and some associated utilities are in the
          <a href="https://github.com/iechevarria/lego-face-VAE/tree/master/ml">ml</a>
          directory of my project repo.
        </p>

        <h2 id="training-set">The training set</h2>
        <p>
          I threw together some quick scripts to pull images of Lego faces from
          two sources: Bricklink and Quartz. For the uninitiated, Bricklink is
          a marketplace for Lego parts and sets. I made a simple scraper to get
          the URL of the largest image for each Lego head on the site and to
          save that image locally.
        <p>
          Quartz, meanwhile, published an
          <a href="https://qz.com/1405657/how-are-lego-emotions-changing-help-us-find-out/">article/piece of interactive software</a>
          by <a href="https://twitter.com/sadbumblebee">Daniel Wolfe</a>
          about the changing emotions of Lego faces. It asked users to label the
          emotions in around 1600 Lego faces to see how they've changed since 
          1975. The photos were taken by
          <a href="http://www.bartneck.de/">Christoph Bartneck, Ph.D</a>, a
          researcher and Lego fan, and they're all excellent. Fortunately, the
          Quartz application had a very simple-to-use API that allowed me to
          pull the images without resorting to full-blown scraping. (Note:
          scraping HTML directly should be your last resort - be a good internet
          citizen and use APIs when they exist.)
        </p>
        <p>
          Once I had all the images in one place, I made some quick scripts to
          make them uniform. For example, several hundred of the Bricklink
          images contained two or three photos side-by-side, so I made a script
          to identify and split them into individual photos. The scripts I used
          to pull and process the images are in the 
          <a href="https://github.com/iechevarria/lego-face-VAE/tree/master/dataset_scripts">dataset_scripts directory</a>
          in my project repo.
        </p>
        <p>
          I eventually reached the end of what I could reasonably automate and
          had to manually pick through the images. I removed several hundred
          images for quality issues including bad lighting and strange angles.
          Several hundred others did not contain faces. In the end, I came out
          with a 3800 image training set, and I had some concern that its small
          size would preclude me from doing anything interesting with it.
        </p>
        
        <h2 id="results">Results</h2>
        <p>
          I trained the model on images from the training set downsized from
          128x128 to 64x64 (only 4 MB of training data total). The VAE's encoder
          has 5 convolutional layers and its decoder
          has 5 convolutional transpose layers. I used 200 dimensions for the
          latent vector. The overall model has
          2.7 million parameters and weighs in at 10 MB when trained. I trained
          it for two hours on a Google Colab GPU instance. 
        </p>
        <p>
          I made no special effort to improve the VAE's performance or to choose
          an optimal architecture. I recognize the the number of layers and the
          size of the latent vector is probably overkill for this task. My sole
          aim was to get a usable model to do the fun stuff below.
        </p>

        <h3 id="image-reconstructions">Image reconstructions</h3>
        <p>
          The following shows images and their reconstructions:
        </p>
        <img class="img-no-pad" src="./blog/lego-face-vae/reconstruction.png" alt="Lego face reconstructions"/>
        <div class="img-caption">
          <ul>
            <li>
              <b>Top</b>: raw input images
            </li>
            <li>
              <b>Bottom</b>: reconstructions created by converting the raw
              input image into a latent vector with the VAE encoder and then
              converting this latent vector into an image with the VAE decoder 
            </li>
          </ul>
        </div>
        <p>
          The results are okay! At a high level, it looks like the model gets
          that a minifig's face should have eyes, eyebrows, and a mouth. It
          also renders the shape, color, and lighting of the heads with fair
          accuracy.
        </p>
        <p>
          More interesting than what the model gets right is what it gets wrong.
          Across the board, it has trouble with expressions. Take the first
          raw image - the face is an angry, mustached, and goateed. Meanwhile,
          its reconstruction lacks any kind of facial hair, and it has a an
          expression that's closer to
          <a href="https://tvtropes.org/pmwiki/pmwiki.php/Main/DreamWorksFace">DreamWorks Face</a>
          than anger.
        </p>
        <p>
          The last raw image and reconstruction go in the opposite direction:
          from happy to angry, or at least displeased. The raw image is a simple
          smiley face with eyebrows. You'd think it'd be hard to mess up, but
          its reconstruction is much sterner, with a frown that approaches a
          sneer.
        </p>
        <p>
          If your initial reaction to all this is "okay, so you managed to take
          4 MB of images and store it in 10 MB of model and still end up with
          terrible reproductions" - you're right! Luckily for us, VAEs can do
          more than encode and decode single images.
        </p>

        <h3 id="face-morphing">Face morphing</h3>
        <p>
          Look at the five following images. Seriously, look at them for a
          bit. Hopefully you noticed that they're morphing between two different
          faces. Neat, right?
        </p>
        <div style="text-align: center;">
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="./blog/lego-face-vae/face-morph-1.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="./blog/lego-face-vae/face-morph-2.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="./blog/lego-face-vae/face-morph-3.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="./blog/lego-face-vae/face-morph-4.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="./blog/lego-face-vae/face-morph-5.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>  
        </div>
        <br>
        <p>
          The mechanics are pretty simple. First, we take two images that we
          want to morph between. We put them into the VAE's encoder and get
          a latent vector out for each. Recall from earlier that the latent
          vector is essentially a point in space. Next, we take the two vectors
          and get several intermediate vectors on the line between them.
          Finally, we take all the vectors and pass them into the VAE's decoder
          to generate images. The following image depicts this process:
        </p>
        <div style="text-align: center;">
          <img class="img-horiz img-really-no-pad" style="width: 18em;" src="./blog/lego-face-vae/morph-example-1.jpg" alt="t-SNE plot"/>
          <img class="img-horiz img-really-no-pad" style="width: 18em;" src="./blog/lego-face-vae/morph-example-2.jpg" alt="t-SNE plot"/>
        </div>
        <div class="img-caption">
          <ul>
            <li>
              <b>Left</b> (top on mobile): two initial images are assigned
              latent vectors by the VAE's encoder. We then choose several 
              intermediate vectors (each represented by
              <b style="color: blue;">o</b>) between these two images'
              latent vectors
            </li>
            <li>
              <b>Right</b> (bottom on mobile): the VAE's decoder turns the
              chosen vectors into images
            </li>
          </ul>
        </div>
        <p>
          Note that none of the faces in the transition between the two input
          faces exist in the training set. These are brand new Lego faces that
          nobody has ever seen before!
        </p>
        <p>
          The randomness ensures that it's a smooth transition
        </p>

        <h3 id="generating-new-faces">Generating new faces</h3>
        <p>
          Generating new faces follows a similar process to face morphing.
          Instead of starting out with the encoder, though, we jump right into
          the decoder. First, we get a vector with random values. Then we pass
          the vector into the VAE's decoder and get a brand new image out that
          most likely does not exist in the training set. The following image
          depicts this process:
        </p>
        <div style="text-align: center;">
          <img class="img-horiz img-really-no-pad" style="width: 18em;" src="./blog/lego-face-vae/generative-example-1.jpg" alt="t-SNE plot"/>
          <img class="img-horiz img-really-no-pad" style="width: 18em;" src="./blog/lego-face-vae/generative-example-2.jpg" alt="t-SNE plot"/>
        </div>
        <div class="img-caption">
          <ul>
            <li>
              <b>Left</b> (top on mobile): we choose random vectors in
              the latent space (each represented by <b style="color: blue;">o</b>)
            <li>
              <b>Right</b> (bottom on mobile): the VAE's decoder turns the
              chosen vectors into images
            </li>
          </ul>
        </div>
        <p>
          So how well does this work in practice? The results range from
          nightmarish to somewhat convincing. The following are faces generated
          randomly by the process described above:
        </p>
        <img class="img-really-no-pad" src="./blog/lego-face-vae/random-1.png" alt="Randomly generated Lego faces"/>
        <img class="img-really-no-pad" src="./blog/lego-face-vae/random-3.png" alt="Randomly generated Lego faces"/>
        <img class="img-really-no-pad" src="./blog/lego-face-vae/random-4.png" alt="Randomly generated Lego faces"/>
        <img class="img-really-no-pad" src="./blog/lego-face-vae/random-5.png" alt="Randomly generated Lego faces"/>
        <img class="img-really-no-pad" src="./blog/lego-face-vae/random-6.png" alt="Randomly generated Lego faces"/>
        <p>
          Nearly all the generated images have the standard features of a Lego
          face: eyes, eyebrows, and a mouth. They almost all get the shape of a
          Lego head right, too.
        </p>
  
        <h3 id="visualizing-latent-space">Visualizing the latent space</h3>
        <p>
          It would be neat to see all the Lego faces' positions in the latent
          space, but it has 200 dimensions. It's usually inadvisable to plot
          data in three dimensions, let alone 200.
        </p>
        <p>
          Fortunately, there exist many methods for dimensionality reduction.
          One such method is <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-SNE</a>
          (t-distributed stochastic neighbor embedding). I'm not going to get
          into the details of how it works because that's well out of scope for
          this post, but the upshot is this: we can use t-SNE to take
          200-dimensional vectors and to turn them into 2-dimensional vectors
          while minimizing the amount of information we lose.
        </p>
        <p>
          The following image contains all the images in the training set:
        </p>
        <a href="./blog/lego-face-vae/tsne-all-lg.jpg">
          <img class="img-really-no-pad" src="./blog/lego-face-vae/tsne-all-sm.jpg" alt="t-SNE plot of all the images"/>
        </a>
        <p>
          The following are some crops from the full plot that demnstrate how
          faces with similar features cluster together:
        </p>
        <div style="text-align: center;">
          <img class="img-horiz" style="width: 11em; border: 1px solid black;" src="./blog/lego-face-vae/tsne-crop-1.jpg" alt="t-SNE plot"/>
          <img class="img-horiz" style="width: 11em; border: 1px solid black;" src="./blog/lego-face-vae/tsne-crop-2.jpg" alt="t-SNE plot"/>
          <img class="img-horiz" style="width: 11em; border: 1px solid black;" src="./blog/lego-face-vae/tsne-crop-3.jpg" alt="t-SNE plot"/>
        </div>
        <p>
          In the first image, you can see that faces with black hair and
          mustaches are in a group. In the second image, four heads
          with dark balaclavas and white headbands sit close to one another. In
          the last image, heads with orange visors (and a jack-o-lantern?) are
          clustered together. The
          <a href="./blog/lego-face-vae/tsne-all-lg.jpg">full-size image</a>
          has other little clusters like these.
        </p>
        <p>
          A word of caution: it's important not to assign too much meaning to
          this visualization or any made using t-SNE. Because the latent vectors
          have been compressed so severely, we are unable to see many of the
          distinctions that the model actually makes. Still, we can begin to
          build some intuition about what the model understands about faces.
          And I think it looks neat, too!
        </p>

        <h2>Parting thoughts</h2>
        <p>
          I hope this was an interesting look at variational autoencoders! Again,
          all of the data and code I used to make the model, plots, and animations
          in this post are publicly available in the project
          <a href="https://github.com/iechevarria/lego-face-VAE">repo</a>.
          The project repo also includes a quickstart guide to running the code
          on <a href="https://colab.research.google.com/">Google Colab</a>, a
          free environment to run Jupyter Notebooks with free optional GPU/TPU
          instances.
        </p>
        
        <p class="note">
          If you want to buy the book <em>Generative Deep Learning</em> from 
          Amazon and you want to throw a buck my way, here's an 
          <a href="https://amzn.to/2RBqkBl">affiliate link</a>. I didn't write
          this post to sell you anything, I just liked the book. None of the
          other links in this post are affiliate links.
        </p>

        <p class="footer">Ivan Echevarria - January 29, 2020 </p>
      </div>
    </div>
  </div>

  <script data-goatcounter="https://ivan.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
</body>
	  