<head>
  <title>Variational autoencoder for Lego faces - echevarria.io</title>
  <base href="../../"/>
  <link rel="stylesheet" type="text/css" href="css/default.css">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>

<body>
  <div class="wrapper" id="main-wrapper">
    <div class="nav">
      <a class="home" href="index.html">Ivan Echevarria</a>
      <div class="spacer"></div>
      <a href="photos/index.html">photos</a>
      <a href="code/index.html">code</a>
      <a href="blog/index.html">blog</a>
      <a href="about.html">about</a>
    </div>
    <div class="content">

      <div class="blog-post">
        <h1>Variational autoencoder for Lego faces</h1>
        <img class="img-no-pad" src="./blog/lego-face-vae/face-morph-1.png" alt="Lego face morph visualization"/>
        <img class="img-no-pad" src="./blog/lego-face-vae/face-morph-2.png" alt="Lego face morph visualization"/>
        <img class="img-no-pad" src="./blog/lego-face-vae/face-morph-3.png" alt="Lego face morph visualization"/>
        <img class="img-no-pad" src="./blog/lego-face-vae/face-morph-4.png" alt="Lego face morph visualization"/>
        <img src="./blog/lego-face-vae/face-morph-5.png" alt="Lego face morph visualization"/>
        <p>
          I spent some of my time off this winter reading 
          <a href="https://github.com/davidADSP">David Foster</a>'s excellent book
          <a href="https://www.oreilly.com/library/view/generative-deep-learning/9781492041931/"><em>Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play</em></a>
          and working through some of the exercises in it.
        </p>
        <p>
          I learn best by doing, so I decided to try to write my own code to
          build a Variational Autoencoder.
        </p>
        <p>
          At a certain point, working with <a href="">MNIST</a>, the hello-world
          of machine learning, or any other pre-made training set gets old.
          The result is assured from the start, which is great when just
          starting out, so there are few surprises.
        </p>
    
        <h2>The high-level, handy-wavy explanation of how a variational autoencoder works</h2>
        <p>
          An autoencoder (AE) has two main parts: an encoder and a decoder. The
          encoder takes high-dimensional input (e.g., an image) and compresses
          it into a low-dimenstional vector representation which I'll refer to
          as a latent vector. You can think of a latent vector as a point
          in a space that it shares with other latent vectors. I'll refer to
          this space as the latent space. Meanwhile, the decoder takes a latent
          vector as input and expands it back to form of the original input.
        </p>
        <em>need to explain latent space better</em>
        <p>
          The following image shows Lego faces and their positions in latent
          space. This is a simplified example. While this graph only shows two
          dimensions, we'd actually likely use many more in a real autoencoder.
        </p>
        <div style="text-align: center;">
          <img style="width: 25em;" src="./blog/lego-face-vae/tsne-example.jpg" alt="t-SNE plot"/>
        </div>
        <p>
          During training, the encoder and decoder are connected, and their
          weights are optimized to minimize the difference between training
          image that enters the encoder and each reconstructed
          image that exits the decoder. The following diagram shows the process
          of an image being compressed into a latent vector by the encoder and
          then being reconstructed by the decoder:
        </p>
        <img src="./blog/lego-face-vae/vae.svg" alt="Diagram of an autoencoder"/>
        <p>
          A vanilla AE's encoder is connected directly and deterministically to
          the latent vector. Variational autencoders (VAEs) change things up by
          having their encoders define a distribution from which the latent
          vector will be randomly drawn. When a VAE trains, it not only
          minimizes the difference between input images and reconstructions, but
          it also minimizes the divergence between the encoder's distribution
          and the normal distribution.
        </p>
        <p>
          The upshot of this added randomness is that when two nearby vectors
          are put into a VAE's decoder, the reconstructed images are also
          similar. No such assurance exists for a vanilla AE. This property
          gives us the ability to do some fun stuff with a VAE that we'll get
          into later.
        </p>
        <p>
          If you want a bit more detail, check out 
          <a href="http://kvfrans.com/">Kevin Frans</a>'
          <a href="http://kvfrans.com/variational-autoencoders-explained/">blog post</a>.
          If you're looking for a more in-depth discussion of the theory and
          math behind VAEs, 
          <em><a href="https://arxiv.org/abs/1606.05908">Tutorial on Variational Autoencoders</a></em>
          by <a href="http://www.carldoersch.com/">Carl Doersch</a> is quite
          thorough.
        </p>


        <h2>The training set</h2>
        <p>
          I threw together some quick scripts to pull images of Lego faces from
          two sources: Bricklink and Quartz. For the uninitiated, Bricklink is
          a marketplace for Lego parts and sets. I made a simple scraper to get
          the URL of the largest image for each Lego head on the site and to
          save that image locally.
        <p>
          Quartz, meanwhile, published an
          <a href="https://qz.com/1405657/how-are-lego-emotions-changing-help-us-find-out/">article/piece of interactive software</a>
          by <a href="https://twitter.com/sadbumblebee">Daniel Wolfe</a>
          about the changing emotions of Lego faces. It asked users to label the
          emotions in around 1600 Lego faces to see how they've changed since 
          1975. The photos were taken by
          <a href="http://www.bartneck.de/">Christoph Bartneck, Ph.D</a>, a
          researcher and Lego fan, and they're all excellent. Fortunately, the
          Quartz application had a very simple-to-use API that allowed me to
          pull the images without resorting to full-blown scraping. (Note:
          scraping HTML directly should be your last resort - be a good internet
          citizen and use APIs when they exist.)
        </p>
        <p>
          Once I had all the images in one place, I made some quick scripts to
        </p>
        <p>
          The scripts I used to pull and process the images are in the 
          <a href="https://github.com/iechevarria/lego-face-VAE/tree/master/dataset_scripts">dataset_scripts directory</a>
          in my project repo.
        </p>
        <h2>Results</h2>
        <p>
          I trained the model on images from the training set downsized from
          128x128 to 64x64 (only 4 MB of training data total). The VAE's encoder
          has 5 convolutional layers and its decoder
          has 5 convolutional transpose layers. I used 200 dimensions for the
          latent vector. The overall model has
          2.7 million parameters and weighs in at 10 MB when trained. I trained
          it for two hours on a Google Colab GPU instance. 
        </p>
        <p>
          I made no special effort to improve the VAE's performance or to choose
          an optimal architecture. I recognize the the number of layers and the
          size of the latent vector is probably overkill for this, but 
        </p>

        <h3>Image reconstructions</h3>
        <p>
          The following image shows randomly selected raw input images the top
          row and the model's reconstructions on the bottom row:
        </p>
        <img class="img-no-pad" src="./blog/lego-face-vae/reconstruction.png" alt="Lego face reconstructions"/>
        <p>
          The results are okay! At a high level, it looks like the model gets
          that a minifig's face should have eyes, eyebrows, and a mouth. It
          also renders the shape, color, and lighting of the heads with fair
          accuracy.
        </p>
        <p>
          More interesting than what the model gets right is what it gets wrong.
          Across the board, it has trouble with expressions. Take the first
          raw image - it's an angry, mustached, goateed face. Meanwhile, its
          reconstruction lacks any kind of facial hair, and it has a an
          expression that's closer to
          <a href="https://tvtropes.org/pmwiki/pmwiki.php/Main/DreamWorksFace">DreamWorks Face</a>
          than anger.
        </p>
        <p>
          The last raw image and reconstruction go in the opposite direction:
          from happy to angry, or at least displeased. The raw image is a simple
          smiley face with eyebrows. You'd think it'd be hard to mess up, but
          its reconstruction is much sterner, with a frown that approaches a
          sneer.
        </p>
        <p>
          If your initial reaction to all this is "okay, so managed to take 4 MB
          of images and store it into 10 MB of model and still end up with
          terrible reproductions" - you're right. Luckily for us, VAEs can do
          more than encode and decode single images.
        </p>

        <h3>Face morphing</h3>
        <p>
          Look at the five following images. Seriously, look at them for a
          bit. Hopefully you noticed that they're morphing between two different
          faces. Neat, right?
        </p>
        <div style="text-align: center;">
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="./blog/lego-face-vae/face-morph-1.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="./blog/lego-face-vae/face-morph-2.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="./blog/lego-face-vae/face-morph-3.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="./blog/lego-face-vae/face-morph-4.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="./blog/lego-face-vae/face-morph-5.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>  
        </div>
        <br>
        <p>
          The mechanics are pretty simple. First, we take two images that we
          want to morph between. We put them into the VAE's encoder and get
          a latent vector out for each. Recall from earlier that the latent
          vector is essentially a point in space. Next, we take the two vectors
          and get several intermediate vectors on the line between them.
          Finally, we take all the vectors and pass them into the VAE's decoder
          to generate images. The following image depicts this process:
        </p>
        <div style="text-align: center;">
          <img class="img-horiz" style="width: 18em;" src="./blog/lego-face-vae/morph-example-1.jpg" alt="t-SNE plot"/>
          <img class="img-horiz" style="width: 18em;" src="./blog/lego-face-vae/morph-example-2.jpg" alt="t-SNE plot"/>
        </div>
        <p>
          The left image shows the two initial images being assigned latent
          vectors by the VAE's encoder. It also shows the process of choosing
          points in between these two images. The right image shows the VAE
          decoder's output for each of the points.
        </p>
        <p>
          Note that none of the faces in the transition between the two input
          faces exist in the training set. These are brand new Lego faces that
          nobody has ever seen before!
        </p>
        <p>
          The randomness ensures that it's a smooth transition
        </p>

        <h3>Generating new faces</h3>
        <p>
          Generating new faces follows a similar process to face morphing.
          Instead of starting out with the encoder, though, we jump right into
          the decoder. First, we get a vector with random values. Then we pass
          the vector into the VAE's decoder and get a brand new image out that
          most likely does not exist in the training set. The following image
          depicts this process:
        </p>
        <div style="text-align: center;">
          <img class="img-horiz" style="width: 18em;" src="./blog/lego-face-vae/generative-example-1.jpg" alt="t-SNE plot"/>
          <img class="img-horiz" style="width: 18em;" src="./blog/lego-face-vae/generative-example-2.jpg" alt="t-SNE plot"/>
        </div>
        <p>The image on the left show how we randomly sample points</p>
        <p>
          So how well does this really work? The results range from nightmarish
          to fairly convincing.
        </p>
        <img class="img-really-no-pad" src="./blog/lego-face-vae/random-1.png" alt="Randomly generated Lego faces"/>
        <img class="img-really-no-pad" src="./blog/lego-face-vae/random-2.png" alt="Randomly generated Lego faces"/>
        <img class="img-really-no-pad" src="./blog/lego-face-vae/random-3.png" alt="Randomly generated Lego faces"/>
        <img class="img-really-no-pad" src="./blog/lego-face-vae/random-4.png" alt="Randomly generated Lego faces"/>
        <img class="img-really-no-pad" src="./blog/lego-face-vae/random-5.png" alt="Randomly generated Lego faces"/>
        <img class="img-really-no-pad" src="./blog/lego-face-vae/random-6.png" alt="Randomly generated Lego faces"/>


        <h2>Parting thoughts</h2>
        <p>
        </p>

        <p class="note">
          If you want to buy the book <em>Generative Deep Learning</em> from 
          Amazon and you want to throw a buck my way, here's an 
          <a href="https://amzn.to/2RBqkBl">affiliate link</a>. I didn't write
          this post to sell you anything, I just liked the book. None of the
          other links in this post are affiliate links.
        </p>

        <p class="footer">Ivan Echevarria - January 29, 2020 </p>
      </div>
    </div>
  </div>

  <script data-goatcounter="https://ivan.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
</body>
	  