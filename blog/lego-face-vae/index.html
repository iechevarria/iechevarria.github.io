<head>
  <title>Variational autoencoder for Lego faces - echevarria.io</title>
  <base href="../../"/>
  <link rel="stylesheet" type="text/css" href="css/default.css">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>

<body>
  <div class="wrapper" id="main-wrapper">
    <div class="nav">
      <a class="home" href="index.html">Ivan Echevarria</a>
      <div class="spacer"></div>
      <a href="photos/index.html">photos</a>
      <a href="code/index.html">code</a>
      <a href="blog/index.html">blog</a>
      <a href="about.html">about</a>
    </div>
    <div class="content">
      <div class="blog-post">
        <h1>Variational autoencoder for Lego faces</h1>
        <img class="img-no-pad" src="./blog/lego-face-vae/face-morph-1.png" alt="Lego face morph visualization"/>
        <img class="img-no-pad" src="./blog/lego-face-vae/face-morph-2.png" alt="Lego face morph visualization"/>
        <img class="img-no-pad" src="./blog/lego-face-vae/face-morph-3.png" alt="Lego face morph visualization"/>
        <img class="img-no-pad" src="./blog/lego-face-vae/face-morph-4.png" alt="Lego face morph visualization"/>
        <img src="./blog/lego-face-vae/face-morph-5.png" alt="Lego face morph visualization"/>
        <p>
          I spent some of my time off this winter reading 
          <a href="https://github.com/davidADSP">David Foster</a>'s excellent book
          <a href="https://www.oreilly.com/library/view/generative-deep-learning/9781492041931/"><em>Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play</em></a>.
          The book is about generative models: models that learn to describe how
          data sets are structured and that can be used to create
          new data. Some of the specific models it covers include autoencoders,
          generative adversarial networks (GANs), and recurrent neural networks
          (RNNs).
        </p>
        <p>
          I decided to try write a model described in the book on my own, and I
          went with a variational autoencoder (VAE) because they train
          relatively quickly. Because of this, I wouldn't have to wait long to
          figure out if I had made the model right or mangled it.
          I also wanted to do something kind of novel with the model, so I
          compiled a new data set for the it to train on: 3800 images of Lego
          minifig faces. The end product can be used for interesting
          applications including generating new Lego faces and morphing smoothly
          between Lego faces.
        </p>
        <p>
          This post will cover the following topics:
          <ul>
            <li>
              <a href="./blog/lego-face-vae/index.html#vae-explained">Background information about VAEs</a>
            </li>
            <li>
              <a href="./blog/lego-face-vae/index.html#implementation">Implementation details</a>
              (<a href="./blog/lego-face-vae/index.html#training-set">training set</a>,
                <a href="./blog/lego-face-vae/index.html#model-training">model training</a>)
            </li>
            <li>
              <a href="./blog/lego-face-vae/index.html#applications">Model applications</a>
              (<a href="./blog/lego-face-vae/index.html#image-reconstruction">image reconstruction</a>,
                <a href="./blog/lego-face-vae/index.html#generating-new-images">generating new images</a>,
                <a href="./blog/lego-face-vae/index.html#image-morphing">image morphing</a>,
                <a href="./blog/lego-face-vae/index.html#visualizing-latent-space">visualizing the latent space</a>)
            </li>
          </ul>
        </p>
        <p>
          All of the data and code I used to make the model, plots, and animations
          in this post are publicly available in the project
          <a href="https://github.com/iechevarria/lego-face-VAE">repo</a>.
        </p>
  
        <h2 id="vae-explained">Background: the high-level, handy-wavy explanation of how a variational autoencoder works</h2>
        <p>
          This section covers what a VAE is and what it does at a high level.
          If you already know how a VAE works, you can safely skip this section.
        </p>
        <p>
          An autoencoder (AE) has two main parts: an encoder and a decoder. The
          encoder takes high-dimensional input, an image in this case, and maps it
          to a point in a low-dimensional space. The point is called a latent
          vector, and the low-dimensional space is called the latent space.
          Ideally, each coordinate in the latent space encodes some kind of
          information - you could imagine that for the case of images of Lego
          faces, one coordinate might indicate face color while another might
          indicate whether or not a face has glasses, and so on. The following
          shows an example of how an AE's encoder might map images to a latent space:
        </p>
        <div style="text-align: center;">
          <img style="width: 25em;" src="./blog/lego-face-vae/tsne-example.jpg" alt="t-SNE plot"/>
        </div>
        <div class="img-caption">
          <li>
            <b>Above</b>: this graph represents the latent space, and the images
            are in positions output by the encoder. Each image's
            position is defined by a latent vector.
          </li>
        </div>
        <p>
          The decoder, on the other hand, takes a latent vector from the latent
          space as input and expands it back to form of the original input.  
        </p>
        <p>
          During training, the encoder and decoder are connected, and their
          weights are optimized to minimize the difference between each training
          image that enters the encoder and each reconstructed
          image that exits the decoder. The following diagram shows the process
          of an image being compressed into a latent vector by the encoder and
          then being reconstructed by the decoder:
        </p>
        <img src="./blog/lego-face-vae/vae.svg" alt="Diagram of an autoencoder"/>
        <p>
          A vanilla AE's encoder is connected directly and deterministically to
          the latent vector. Variational autencoders (VAEs) change things up by
          having their encoders define a distribution from which the latent
          vector will be randomly drawn. When a VAE trains, it not only
          minimizes the difference between input images and reconstructions, but
          it also minimizes the divergence between the encoder's distribution
          and the normal distribution.
        </p>
        <p>
          The upshot of this added randomness is that when two nearby vectors
          are put into a VAE's decoder, the reconstructed images are also
          similar. No such assurance exists for a vanilla AE. This property
          gives you the ability to do some fun stuff with a VAE that I'll get
          into later.
        </p>
        <p>
          If you want a bit more detail, check out 
          <a href="http://kvfrans.com/">Kevin Frans</a>'
          <a href="http://kvfrans.com/variational-autoencoders-explained/">blog post</a>.
          If you're looking for a more in-depth discussion of the theory and
          math behind VAEs, 
          <em><a href="https://arxiv.org/abs/1606.05908">Tutorial on Variational Autoencoders</a></em>
          by <a href="http://www.carldoersch.com/">Carl Doersch</a> is quite
          thorough.
        </p>

        <h2 id="implementation">Implementation details</h2>
        <p>
          This section covers the specifics of the VAE I made for images of Lego faces.
          It includes a description of how I obtained and curated the training set.
          It also includes some discussion of the model's structure and how I trained it.
        </p>

        <h3 id="training-set">The training set</h3>
        <p>
          I threw together some quick scripts to pull images of Lego faces from
          two sources:
        <ul>
          <li>
            Bricklink, a marketplace for Lego parts and sets. I made a simple
            scraper to get the URL of the largest image for each Lego head on
            the site and to save that image locally.
          </li>
          <li>
            An
            <a href="https://qz.com/1405657/how-are-lego-emotions-changing-help-us-find-out/">article/piece of interactive software</a>
            by <a href="https://twitter.com/sadbumblebee">Daniel Wolfe</a>
            about the changing emotions of Lego faces. It included 1600 photos of Lego faces
            taken by
            <a href="http://www.bartneck.de/">Christoph Bartneck, Ph.D</a>, a
            researcher and Lego fan, and they're all excellent. Fortunately, the
            application had a very simple-to-use API that allowed me to
            pull the images without resorting to full-blown scraping.
          </li>
        </ul>
        <br>
        <p>
          Once I had all the images in one place, I wrote some quick scripts to
          make them uniform. For example, several hundred of the Bricklink
          images contained two or three photos side-by-side, so I made a script
          to identify and split them into individual photos. The scripts I used
          to pull and process the images are in the 
          <a href="https://github.com/iechevarria/lego-face-VAE/tree/master/dataset_scripts">dataset_scripts directory</a>
          in my project repo.
        </p>
        <p>
          I eventually reached the end of what I could reasonably automate and
          had to manually pick through the images. I removed several hundred
          images for quality issues including bad lighting and strange angles.
          Several hundred others did not contain faces. In the end, I had
          a 3800 image training set, and I was a bit concerned that its small
          size would preclude me from doing anything interesting with it.
        </p>
  
        <h3 id="model-details">Model details</h3>
        <p>
          I trained the model on images from the training set downsized from
          128x128 to 64x64 (only 4 MB of training data total). The VAE's encoder
          has 5 convolutional layers and its decoder
          has 5 convolutional transpose layers. I used 200 dimensions for the
          latent vector. The overall model has
          2.7 million parameters and weighs in at 10 MB when trained. I trained
          it for two hours on a Google Colab GPU instance. 
        </p>
        <p>
          I made no special effort to improve the VAE's performance or to choose
          an optimal architecture. I recognize the the number of layers and the
          size of the latent vector is probably overkill for this task. My sole
          aim was to get a usable model to do the fun stuff below.
        </p>
        <p>
          My VAE implementation and some associated utilities are in the
          <a href="https://github.com/iechevarria/lego-face-VAE/tree/master/ml">ml</a>
          directory of my project repo. The code is largely based on snippets
          from the book <em>Generative Deep Learning</em> by David Foster.
          I also put a pretrained model in the
          <a href="https://github.com/iechevarria/lego-face-VAE/tree/master/trained_model">trained_model</a>
          directory of my project repo.
        </p>


        <h2 id="applications">Model applications</h2>
        <p>
          This section covers the neat things that you can do with the trained
          VAE, including generating new images and morphing between existing
          images.
        </p>
  
        <h3 id="image-reconstruction">Image reconstruction</h3>
        <p>
          A quick way to assess the quality of a VAE is to compare raw images
          with their reconstructed counterparts. The following shows images of
          Lego faces and their reconstructions:
        </p>
        <img class="img-no-pad" src="./blog/lego-face-vae/reconstruction.png" alt="Lego face reconstructions"/>
        <div class="img-caption">
          <ul>
            <li>
              <b>Top</b>: raw input images
            </li>
            <li>
              <b>Bottom</b>: reconstructions created by converting the raw
              input image into a latent vector with the VAE encoder and then
              converting this latent vector into an image with the VAE decoder 
            </li>
          </ul>
        </div>
        <p>
          The results are okay! At a high level, it looks like the model gets
          that a minifig's face should have eyes, eyebrows, and a mouth. It
          also renders the shape, color, and lighting of the heads with fair
          accuracy.
        </p>
        <p>
          More interesting than what the model gets right is what it gets wrong.
          Across the board, it has trouble with expressions. Take the first
          raw image - the face is an angry, mustached, and goateed. Meanwhile,
          its reconstruction lacks any kind of facial hair, and it has a an
          expression that's closer to
          <a href="https://tvtropes.org/pmwiki/pmwiki.php/Main/DreamWorksFace">DreamWorks Face</a>
          than anger.
        </p>
        <p>
          The last raw image and reconstruction go in the opposite direction:
          from happy to angry, or at least displeased. The raw image is of a simple
          smiley face with eyebrows. You'd think it'd be hard to mess up, but
          its reconstruction is much sterner, with a frown that approaches a
          sneer.
        </p>
        <p>
          If your initial reaction to all this is "okay, so you managed to take
          4 MB of images and store it in 10 MB of model and still end up with
          terrible reproductions" - you're right! Luckily, VAEs can do a lot
          more than encode and decode single images.
        </p>
  
        <h3 id="generating-new-images">Generating new images</h3>
        <p>
          A VAE is called a generative model because you can use it to create new
          images it's never seen before. So let's make some! The mechanics are
          pretty simple. First, you choose a random vector from the latent space.
          Then you pass this vector into the VAE's decoder and get a brand new 
          image out that most likely does not exist in the training set. The
          following image depicts this process:
        </p>
        <div style="text-align: center;">
          <img class="img-horiz img-really-no-pad" style="width: 18em;" src="./blog/lego-face-vae/generative-example-1.jpg" alt="t-SNE plot"/>
          <img class="img-horiz img-really-no-pad" style="width: 18em;" src="./blog/lego-face-vae/generative-example-2.jpg" alt="t-SNE plot"/>
        </div>
        <div class="img-caption">
          <ul>
            <li>
              <b>Left</b> (top on mobile): we choose random vectors in
              the latent space (each represented by <b style="color: blue;">o</b>)
            <li>
              <b>Right</b> (bottom on mobile): the VAE's decoder turns the
              chosen vectors into images
            </li>
          </ul>
        </div>
        <p>
          So how well does this work in practice? The results range from
          nightmarish to somewhat convincing. The following are faces generated
          randomly by the process described above:
        </p>
        <img class="img-really-no-pad" src="./blog/lego-face-vae/random-1.png" alt="Randomly generated Lego faces"/>
        <img class="img-really-no-pad" src="./blog/lego-face-vae/random-3.png" alt="Randomly generated Lego faces"/>
        <img class="img-really-no-pad" src="./blog/lego-face-vae/random-4.png" alt="Randomly generated Lego faces"/>
        <img class="img-really-no-pad" src="./blog/lego-face-vae/random-5.png" alt="Randomly generated Lego faces"/>
        <img class="img-really-no-pad" src="./blog/lego-face-vae/random-6.png" alt="Randomly generated Lego faces"/>
        <p>
          Nearly all the generated images have the standard features of a Lego
          face: eyes, eyebrows, and a mouth. They almost all get the shape of a
          Lego head right, too. But geez, when the VAE misses, it really misses.
          Some of the generated images put the facial features in the wrong place
          entirely while others are essentially just noise.
        </p>
  
        <h3 id="image-morphing">Image morphing</h3>
        <p>
          Look at the five following images. Hopefully you noticed that they're
          actually animated and morphing between two different
          faces.
        </p>
        <div style="text-align: center;">
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="./blog/lego-face-vae/face-morph-1.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="./blog/lego-face-vae/face-morph-2.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="./blog/lego-face-vae/face-morph-3.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="./blog/lego-face-vae/face-morph-4.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video> 
          <video width="120" height="120" autoplay loop muted playsinline>
            <source src="./blog/lego-face-vae/face-morph-5.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>  
        </div>
        <br>
        <p>
          The process of morphing between images is a little different from generating new images - instead
          of jumping straight into the decoder, you actually need to use the
          encoder, too. There are four main steps:
          <ol>
            <li>
              Choose two images that you want to morph between
            </li>
            <li>
              Put both images into the VAE's encoder and get a latent vector out for each
            </li>
            <li>
              Choose several intermediate vectors between the two latent vectors
            </li>
            <li>
              Take the intermediate vectors and pass them into the VAE's decoder to generate images
            </li>
          </ol>
        </p>
        <p>
          The following image depicts this process:
        </p>
        <div style="text-align: center;">
          <img class="img-horiz img-really-no-pad" style="width: 18em;" src="./blog/lego-face-vae/morph-example-1.jpg" alt="t-SNE plot"/>
          <img class="img-horiz img-really-no-pad" style="width: 18em;" src="./blog/lego-face-vae/morph-example-2.jpg" alt="t-SNE plot"/>
        </div>
        <div class="img-caption">
          <ul>
            <li>
              <b>Left</b> (top on mobile): two initial images are assigned
              latent vectors by the VAE's encoder. You then choose several 
              intermediate vectors (each represented by
              <b style="color: blue;">o</b>) between these two images'
              latent vectors
            </li>
            <li>
              <b>Right</b> (bottom on mobile): the VAE's decoder turns the
              chosen vectors into images
            </li>
          </ul>
        </div>
        <p>
          Note that none of the faces in the transition between the two input
          faces exist in the training set. These are brand new Lego faces that
          nobody has ever seen before!
        </p>
        <p>
          Let's take a closer look at two of these morphs:
        </p>
        <img class="img-no-pad" src="./blog/lego-face-vae/face-morph-3.png" alt="Lego face morph visualization"/>
        <p>
          Notice how every single intermediate face is completely plausible on
          its own. I don't think I'd be able to guess which of these is real
          and which of these is fake if I didn't already know. Going from left
          to right, you can see the mouth naturally close and the eyebrows
          widen.
        </p>
        <img class="img-no-pad" src="./blog/lego-face-vae/face-morph-4.png" alt="Lego face morph visualization"/>
        <p>
          I think this morph is particularly neat because you can see the first
          face's eyebrows move up and spread out to become hair. Additionally,
          you can see the mustache subtly grow and the smile slowly contract at
          each step.
        </p>
        <p>
          Be aware that not all transitions are as clean or as neat as the ones
          shown above - I specifically chose these because they look good. Still, if you
          run through some combinations on your own, it's likely that you can find morphs
          that look even better than these.
        </p>


        <h3 id="visualizing-latent-space">Visualizing the latent space</h3>
        <p>
          It would be neat to see all the Lego faces' positions in the latent
          space, but the latent space has 200 dimensions. It's
          inadvisable to plot data in three dimensions, let alone 200.
        </p>
        <p>
          Fortunately, there exist many methods for dimensionality reduction.
          One such method is <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-SNE</a>
          (t-distributed stochastic neighbor embedding). I'm not going to get
          into the details of how it works because that's well out of scope for
          this post, but the upshot is this: you can use t-SNE to take
          200-dimensional vectors and to turn them into 2-dimensional vectors
          while minimizing the amount of information you lose.
        </p>
        <p>
          I put all the images in the training set through the VAE encoder to
          get their 200 dimension latent vectors. I then used t-SNE to turn
          these 200 dimension vectors into 2-dimensional vectors. Finally, I
          plotted the input images at these coordinates:
        </p>
        <a href="./blog/lego-face-vae/tsne-all-lg.jpg">
          <img class="img-really-no-pad" src="./blog/lego-face-vae/tsne-all-sm.jpg" alt="t-SNE plot of all the images"/>
        </a>
        <p>
          The following are some crops from the full plot that demnstrate how
          faces with similar features cluster together:
        </p>
        <div style="text-align: center;">
          <img class="img-horiz" style="width: 11em; border: 1px solid black;" src="./blog/lego-face-vae/tsne-crop-1.jpg" alt="t-SNE plot"/>
          <img class="img-horiz" style="width: 11em; border: 1px solid black;" src="./blog/lego-face-vae/tsne-crop-2.jpg" alt="t-SNE plot"/>
          <img class="img-horiz" style="width: 11em; border: 1px solid black;" src="./blog/lego-face-vae/tsne-crop-3.jpg" alt="t-SNE plot"/>
        </div>
        <p>
          In the first image, you can see that faces with black hair and
          mustaches are in a group. In the second image, four heads
          with dark balaclavas and white headbands sit close to one another. In
          the last image, heads with orange visors (and a jack-o-lantern?) are
          clustered together. The
          <a href="./blog/lego-face-vae/tsne-all-lg.jpg">full-size image</a>
          has other little clusters like these.
        </p>
        <p>
          A word of warning: it's important that you don't to assign too much
          meaning to this visualization or really any made using t-SNE. Because the
          latent vectors have been compressed so severely, you are unable to see
          many of the distinctions that the model actually makes. Still, you can
          begin to build some intuition about what the model understands about
          faces. And I think it looks neat, too!
        </p>

        <h2>Closing thoughts</h2>
        <p>
          I hope this was an interesting look at my variational autoencoder
          trained on Lego faces! Again,
          all of the data and code I used to make the model, plots, and animations
          in this post are publicly available in the project
          <a href="https://github.com/iechevarria/lego-face-VAE">repo</a>.
        </p>
        <p>
          The project repo also includes a quickstart guide to running the code
          on <a href="https://colab.research.google.com/">Google Colab</a>, a
          free environment to run Jupyter Notebooks with free optional GPU/TPU
          instances. It takes less than a minute to set up, and once it's ready,
          you can generate your own Lego faces and make your own face morph
          animations.
        </p>
        <p>
          Finally, if you find any errors in this post
          or if you just want to chat, feel free to reach out via email at
          <a href="mailto:ivan@echevarria.io">ivan@echevarria.io</a>. Thanks for
          reading!
        </p>
 
        <p class="note">
          If you want to buy the book <em>Generative Deep Learning</em> from 
          Amazon and you want to throw a buck my way, here's an 
          <a href="https://amzn.to/2RBqkBl">affiliate link</a>. I didn't write
          this post to sell you anything, I just liked the book. None of the
          other links in this post are affiliate links.
        </p>

        <em>
          Thanks to <a href="http://devinlogan.org/">Devin Logan</a> for her
          incredibly helpful input on earlier drafts of this post
        </em>
  
        <p class="footer">Ivan Echevarria - February 3, 2020 </p>
      </div>
    </div>
  </div>

  <script data-goatcounter="https://ivan.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
</body>
	  